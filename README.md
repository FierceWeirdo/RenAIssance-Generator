# RenAIssance-Generator

Problem Description
In recent years, scientists have been dedicated to generating creative products such as poetry, stories, jokes, music, paintings, and so on. In their project Generative Adversarial Networks (GAN), Elgammal et al. built a new system to generate art by learning about styles and deviating from style norms. Astonishingly, human subjects could not distinguish paintings generated by this system from paintings made by contemporary artists (Elgammal et al., 2017). This is where computational creativity comes in – which is the study of building softwares that exhibit behavior that would be deemed creative in humans. 
This project concentrates on a smaller domain. The art style that will be the major area of focus is Renaissance Oil Painting. The proposed development will be making use of GAN-based artificial intelligence that takes in an image and produces that same image but with a Renaissance touch to it. 
A more detailed discussion is presented on all current AIs that turn images into Renaissance Paintings (very limited) in the next section but an overview of the problem here is that the existing AIs work on a neural network that is trained on existing Renaissance Paintings only. So, any output produced is taken from existing paintings which in some way or another diverts into (a kind of, if not direct) racism, purely because the Renaissance Period was a relatively chauvinistic period with very little to no portraits of people who didn’t have access to getting portraits made. 
As can be seen, this is an issue in AI art and this project wishes to tackle this problem with the proposed method.
GAN is primarily an unsupervised or supervised learning task that involves automatic discovery and learning of the various patterns in input data.
The reason for choosing GAN for the Renaissance output is that GANs have been proven to work well with image-to-image programs. They have been applied in turning daytime photos to nighttime, for example. Moreover, the language that will be used for this model is Python and GAN with Python has seen a lot of research in the past few years and with a book (Browlee, 2021) already available for the same, that will be used for reference in this project. Given a training set, this technique learns to generate new data with the same statistics as the training set. The next section discloses more details on how a GAN would be implemented as a method to solve this problem definition.
Method
For this project, a Generative Adversarial Network has been used. This technology uses two 2 sub models (2 neural networks), namely the Generator and the Discriminator. The generator tries to generate the images post-training. The discriminator on the other hand tries to classify examples as either real (from the training domain) or fake (generated by the generator). 
Once the Discriminator starts producing false results more often, it is believed that the generator, which is the main model here, is generating examples that are good enough. Both these sub-models were trained, the generator penalized for creating outputs that the discriminator could detect, and the discriminator used the generative instances as negative training examples and real data as positive training examples. The proposed model here makes use of backpropagation to update the weights of both the Discriminator and the Generator’s neural network to ensure learning.
Previous works have been done to implement art styles on images after training a GAN. CycleGAN (Zhu et. Al, 2017) and pix2pix (Isola et. Al, 2017) are two Generative Adversarial Networks that use image to image translation. The former, for example, was used to turn horse to zebras and vice versa, and turning photographs to Monets, or van Goghs. Pix2pix is a conditional GAN that can be used to convert black and white images to colored, or turn day to night time images. These neural networks however, use a much more complex architecture. CycleGANS have 2 discriminators and 2 generators instead of just 1 of each like this project uses. 
Visually, the method used in this project looks something like this:
The architecture was built using Tensorflow (Abadii et al, 2016) and Keras (Chollet, 2015), with the generator having 22 layers, each with its own tasks and the discriminator having 14 layers. Generator and discriminator losses were determined in order to penalize each during training and optimizer values were set with a learning rate of 0.002, (set after intense experimentation to see which value works throughout the code). Once that is done, the GAN is built using Keras, combining the generator and the discriminator – it is then compiled.
To train the GAN, this example tried to make use of 40,000 Renaissance Painting images for the training. The images were retrieved from the ArtDL database available online (ArtDL, n.d.). However, only 7000 were then made use of (more discussed in Limitations). The data loaded was resized to 244 by 244 pixels in a separate function to reduce strain on the GAN code. This was in alignment with the output data size of the generator and the input data size of the discriminator. This had restricted the data output overall to that size for future generated paintings as well.
The training function starts by preprocessing the input images and defining the batch size and number of epochs for the training process. These were experimented on numerous times and the final value of 230 epochs and 256 batch size was determined. 
It then iterates over each epoch and within each epoch, it iterates over each batch of images. For each batch, it first trains the discriminator using real and fake images, and then trains the generator using random noise. It calculates and prints the average loss for each epoch and saves the generated images every five epochs using a separate function that tries to generate the renaissance painting of a digital image. A test image (original image 1 from Gallery) was used for this purpose. This was done in order to observe the training process and make early changes before fully training the code. 
Once the epochs, learning_rate, and batch size were determined to be the best for the given model, the training was started. The final training of the data took around 49 hours (of uninterrupted RAM time). This is beside the numerous hours it took to finalize the parameters here.
The first few times the code was run with the different parameters, the output images between epoch 0 and 50 looked something like this.
As is visible, there is still a lot of noise in the output images displayed in the ‘Gallery’ section of this page. The reason for that is the lacking ability of the hardware. While this is discussed in detail in the Limitations sections, it is important to note here that this model has only been trained on a few thousand images and GANs tend to need data in tens of thousands before they generate good quality images. Previous works on GANs have made use of MNIST database (Yann et. al, 1998) with 60,000 images, for example. More recent GANs such as the BigGAN (Brock et. al, 2018) made use of 14 million images. 
Given that, the cv2 addWeighted function was used exclusively to fasten the process of image generation. The original image was layered with the trained generator’s predicted image in order to bring about more definition. 

Materials
The following materials have been used throughout this project:
1.	Hardware: A Hewlett-Packard 64-bit laptop with processor 11th Gen Intel(R) Core (TM) i5-1135G7 @ 2.40GHz and 16.0 Gb RAM was used throughout. 
2.	Software: A basic Python IDLE was used throughout this project. The libraries used include Tensorflow, Keras, numpy, and cv2. Tensorflow and Keras were used for the deep learning framework whereas open-cv or cv2 was used for image processing and during the generation of the renaissance portrait. 
3.	Dataset: A part of the ArtDL dataset was used as the training data. (ArtDL, 2023) The data set is composed of 42,479 paintings images. However, only the first 7000 were used. This was modified and dark-skinned portraits were added additionally. Here is a glimpse of the dataset used. 
4.	Additional Materials: For research purpose, the book, Generative Adversarial Network with Python (Brownlee, 2021) where the author exclusively talks about Image-to-Image Translation was required. A digital copy was available online for purchase. The Python website and the tensorflow websites were used additionally to understand how code works for GAN and to implement said code.
Evaluation
This project had 2 different kinds of evaluation done for it:
Does it preserve the Skin Tone? 
One way to evaluate the performance of this model is to assess how well it preserves the skin tone of the people in the output portrait image. Skin tone of the input and output images were compared using a metric such as the Skin Color Similarity Index (SCSI). 
A skin tone detection algorithm was built in Python that makes use of the L*a*b* or LAB color space model, and gets the Skin Color Similarity Index from that. 
This code made use of the cv2 and numpy library, and uses a threshold of 0.09 as the maximum difference in skin tones. 
# Define the skin tone range in LAB color space
lower_skin = np.array([0, 133, 77], dtype=np.uint8)
upper_skin = np.array([255, 173, 127], dtype=np.uint8)
In this glimpse of the code, we are defining any pixel that falls in this range to be considered a skin pixel. The skin pixels are then extracted from the images and the percentage of skin tone in each image is calculated. The skin tone is then compared and if the difference is less than 0.05, then the skin tones are considered to match. 
All of the generated portrait images passed this evaluation. It is to be noted here that the generated images were highly distorted and the tone of the entire image had been processed in such a way that the definition was rather low. Despite that, the skin tone was preserved. 
Figure 10. Evaluation 1 Output
Does it produce good quality outputs? 
A qualitative analysis was conducted based on predefined evaluation criteria, which in this project, included visual quality, style consistency, and preservation of skin tone along with how human-made it appeared. 
For this, feedback was solicited from human raters, who evaluated the output images based on the same criteria as the visual inspection.
This evaluation had 2 sub-surveys. The first survey aimed at trying to find how visually appealing the output data was to the raters. A set of output images was presented to the raters and they answered the first survey which included questions on the appeal of the art, the quality of the output, and if it appeared AI-generated or human-made. Within these output images, there were also the ‘real’ paintings that were actual paintings from the Renaissance period and not generated by the AI. This was to ensure that the set given to the raters was diverse and that the feedback also considered how the viewers felt about original paintings. However, since the output images in this project were not similar to Renaissance paintings, most paintings failed in this evaluation.
        
Figure 11. Sub-survey 1 Set
The second survey produced the input images and eliminated the real paintings from the output set. The AI generated paintings were disclosed to the users and their original images (before the art style was applied to them) were put forth next to them. Then the viewers were asked a second set of questions on how well they thought the AI performed in implementing the art style to the image, if they felt that the structural similarity was maintained, and also if they believed that the skin tone was preserved. The landscape portraits were not a part of this survey.
These surveys had been done on 23 people. The findings were as follows:
   
Figure 12. Evaluation 2 Sub-survey 1 statistics for Portrait Images. *Rated out of 10
As can be seen, the AI generated renaissance paintings scored really low (average 3.5 on a scale of 1 to 10) in terms of how human made they appeared and even lower when scored according to the renaissance effect they have. 
 
Figure 13. Evaluation 2 Sub-survey 1 statistics for Landscape Images. *Rated out of 10
Surprisingly the landscape paintings (Images 5 though 7) performed a lot better than the portraits in this evaluation. The Gallery displays 3 generated landscape paintings. The evaluation from these gave the following results.
 
Figure 14. Evaluation 2 Sub-survey 2 statistics for Portrait Images. *Rated out of 10
As can be seen the portraits performed a lot better in sub-survey 2. However, the human evaluation for skin tone preservation was not at par with the machine evaluation of the same. This could be attributed to the heavy noise in the generated images that somehow disrupt the visual to the naked eye.

Limitations
It was fairly difficult finding a database of the size required for said training. For over a week, the images were being manually downloaded until an online database was found that consisted of around 40,000 Renaissance images. (ArtDL, n.d.) This eased the task by a lot.
Initially, the goal was to train the GAN on 40,000 Renaissance paintings of size 448 by 448. This was what determined the quality of the output and ensured that the generated renaissance paintings had been developed after the training would be reliable. The reason that size was chosen was because it seemed to be small enough to not slow down the training by a lot but also large enough to display a good size for the images.
However, this did not work out. Allocating a tensor of the size (40000, 448, 448, 3) which means 40000 images of size 448 by 448 with 3 color channels, was not possible with the existing hardware. This required approximately 192 GB of RAM compared to the only 16 GB RAM that was available. 
 
Figure 15. Allocation Out of Memory error
To deal with this, the training data size had to be significantly reduced to 7000. The image size was also reduced to half of what was initially planned. Allocating a tensor this size, i.e., (7000, 224, 224, 3), required approximately 8.4 Gb RAM. This worked on the hardware as the rest of the RAM was already being used in other tasks. 
There is a really good possibility that this is the main reason that the quality of the output images is not as expected from the code. That in itself was another limitation. The initial quality of the images was extremely bad and finding the dials that worked for the GAN was a process way longer than expected. The final results do not align with what the end goal was supposed to be.
Moreover, within this art database, was the higher proportion of light-skinned portraits and lowered proportion of portraits that were going to ensure that detected faces aren’t turned to lighter skin tones. While finalizing on the dataset – it became a matter of trading off between the quality of the dataset by focusing on balancing between the skin tones in portraits or increasing the data set size as well. Eventually, since a high number of paintings couldn’t be used anyway, a middle ground was reached.
Gallery
For the generation of renaissance paintings, Creative Commons portraits off the internet were used.
Next Steps 
The current GAN, as mentioned produces images that cannot be considered ‘human-looking’ or ‘artistic’ enough to be considered as ‘art’ – as was ascertained in the evaluation as well.
The next steps for this project look something like this:
1.	Adjusting the GAN's parameters to examine how they effect the output quality on a smaller selection of photos. This is in order to experiment on the existing training data and observe how the outcomes change.
2.	Training the GAN on a larger dataset of images to see if this improves the quality of the output. This could be an interesting experiment to see if the GAN can generate more realistic-looking Renaissance paintings with a larger dataset. This would require finding better hardware for more room for tensor allocation.
3.	Exploring different artistic styles and training the GAN on those images. By doing this, creation of a more diverse range of outputs that could mimic the style of various artists can be achieved.
4.	Another thing we can try with this GAN is to implement a user interface that allows users to upload their own images and have them transformed into Renaissance paintings. This could be a fun way for people to experiment with the GAN and see how it can transform their photos.
5.	The GAN can also be worked around to create a Neural Style Transfer. Transfer learning can be used to adapt the pre-trained GAN to a different image domain.

While all of this seems interesting, the most important thing to focus at here is to make the GAN more efficient. It is vital to work on the current hardware a bit more and see how much the output can be improved, without making any changes to the RAM and the training data.
